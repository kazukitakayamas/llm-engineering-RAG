
---
**内容：メモリ制約下における大規模言語モデル推論能力向上のための強化学習手法に関する研究**

**著者：** Alan Lee, Harry Tong (ミシガン大学 計算機科学科)


T-SPMOは、LLMの強化学習（RL）において「各トークン単位での細粒度なクレジット割り当て」を可能にする手法です。LoRAによるパラメータ効率の高い微調整と組み合わせることで、単一の40 GB GPUなどメモリ制約のある環境下でも高い性能向上を実現します。T-SPMO は “Token-Specific Prefix Matching Optimization” の略称であり、従来のGRPO（Group Relative Policy Optimization）のようにシーケンス全体の統計を取らずに、トークンごとに報酬を割り当てる点が特徴です。

動作原理
1. Prefix treeの構築
まず、各入力プロンプトに対してモデルから複数の出力（completions）をサンプリングします。

これらの出力に基づき、すべてのトークン遷移を表す「prefix → 次のトークン」のペアを集め、前方一致木（prefix tree）を構築します。

2. 再生成と評価
各出力シーケンスについて、ランダムに選んだ位置𝑖（＝リプレイ位置）までの prefix 𝑡0:𝑖−1t 0:i−1を用意します。
その prefix から再度モデルを用いて新しい continuations を生成し、報酬関数で評価します。
得られた報酬をもとに「(prefix, トークン)」ごとのアドバンテージ値を計算し、モデルを更新します。

特徴と利点
トークン単位の細粒度最適化：一度のシーケンス生成全体でなく、個々の prefix → トークン遷移に焦点を当てるため、局所的に正しい判断を強化しやすい。

クリティック不要：PPOのような価値ネットワーク（critic）を必要とせず、計算とメモリの両面で軽量化が可能。

LoRAとの相性：LoRAベースのパラメータ効率チューニングと組み合わせることで、全モデル微調整に比べてメモリ使用量を大幅に削減できる。

適用例と効果
SVAMPベンチマーク：ベースモデルの約46％から70％以上へと大幅改善。

多桁乗算タスク：3桁×3桁の算術問題でベースの3.9％から約70％へ飛躍的に向上し、S-GRPOを大きく上回る結果を示した。

堅牢性：部分的パラメータ更新下でも安定して性能を発揮し、過学習のリスクを低減する正則化効果が期待される 。


選択の指針
トークン精度重視のタスク（例：算術、手続き的推論）ではT-SPMOを推奨。
出力全体の構造的整合性が重要な場合は、群単位最適化のS-GRPOとの組み合わせ検討が有効です。


**概要**

本報告書は、**単一の40GB GPUという一般的な学術環境におけるメモリおよび計算制約下**で、大規模言語モデル（LLMs）の特定の**推論タスクにおける性能を強化学習（RL）によって強化する手法**に関する研究内容を要約したものです。標準的なRL手法（PPO, GRPO）はリソース要求が高く、制約のある環境には不向きであり、また教師ありファインチューニングは高品質データの収集が困難であるという課題があります。本研究ではこれらの課題に対処するため、**メモリ効率が高く、クリティックフリー（critic-free）であり、LoRA（Low-Rank Adaptation）によるパラメータ効率の良いファインチューニングと互換性のある2つの新しいRL手法**、すなわち**S-GRPO**と**T-SPMO**を提案・評価します。これらの手法をQwen2-1.5BモデルにLoRAを用いて適用し、SVAMPデータセットと多桁掛け算タスクで評価した結果、両手法ともにベースモデルの性能を大幅に向上させ、特にT-SPMOは多桁掛け算タスクで顕著な性能向上を示しました。

**1. はじめに**

LLMsは、特定のタスクに対するRLファインチューニングを通じて、数学や構造化された問題解決において大きな進歩を遂げています。しかし、**PPOやGRPOのような従来のRL手法**は、**出力軌道全体に対する損失計算、フルモデルファインチューニングへの適合性、PPOにおけるクリティックネットワークの必要性**といった理由から、限られた計算リソースを持つ研究者にとっては実用的ではありません。また、高品質なChain-of-Thoughtデータの収集が必要な教師ありファインチューニングも、多様な問題領域では高価または非現実的です。本研究は、**パラメータ効率の良い更新と単一の40GB GPUのみを使用して、RLファインチューニングが推論性能を向上させることができるか**という問いを探求します。この目的のために、我々は制約のある環境向けに調整された2つのクリティックフリー手法を開発・評価します。

**2. 提案手法**

本研究では、メモリ制約下でのRLファインチューニングを可能にするため、以下の2つの手法を提案します。問題設定は、プロンプト `X` に対するトークン列 `Y` の条件付き生成として捉え、生成されたシーケンスの正しさや品質に基づいてスカラー報酬 `r(Y)` が与えられます。目的は、期待される報酬を最大化するようにモデルパラメータ `θ` を最適化することです。GRPOは、個別の価値ネットワークなしに、同じプロンプトに対して生成された複数の応答の報酬を比較することでアドバンテージを推定するクリティックフリーな手法です。我々の手法はGRPOのアイデアに基づき、メモリ効率を向上させています。また、パラメータ効率の良いファインチューニングのためにLoRAを採用しています。

**2.1. S-GRPO (Stochastic Group Relative Policy Optimization)**

S-GRPOは、GRPOを低メモリ環境に対応させるために、**応答軌道全体ではなく、サンプリングされたトークンのみを勾配計算に含める**軽量版です。TRLのような軽量RLライブラリの設定に合わせて、問題ごとの更新回数は1回とし、PPOスタイルのクリップされた目的関数や `π_ref` の更新を省略しています。

トークンの損失への貢献方法は以下のルールに基づきます:
*   カットオフインデックス `α` より前の早期のトークンは常に含める。
*   それ以降のトークンは、最大トークン数 `k` を超えない限り、確率 `P` に基づいて確率的にサンプリングする。

このハイブリッドなサンプリングルールにより、**重要なセマンティックステップを含む可能性のある早期のトークンを確実に更新しつつ、後続のトークンをサンプリングすることで、学習の安定性とメモリ効率のバランス**を取ります。これにより、完全な軌道を保持することなく、より大きなバッチサイズへのスケーリングが可能になります。

**2.2. T-SPMO (Token-Specific Prefix Matching Optimization)**

T-SPMOは、**軌道全体の統計情報なしに、よりきめ細かい信用割り当て（credit assignment）を可能にするトークンレベルのRLアルゴリズム**です。S-GRPOと同様に、プロンプトごとに複数の生成候補をサンプリングします。これらの候補からプレフィックスツリーを構築し、ユニークな `(プレフィックス, トークン)` のペア `(p, v)` を特定します。

各 `(p, v)` ペアには、`v` を `p` に追加した場合の期待される報酬の変化に基づいた**トークンレベルのアドバンテージ `A(v|p)`** が割り当てられます。これらの期待値は、サンプリングされた生成候補における経験的平均を用いて近似されます。

ポリシーは、以下の目的関数を用いて更新されます:
`𝒥_TSPMO(θ) = (1/|𝒰|) * Σ_{(p,v)∈𝒰} π_θ(v|p) * A(v|p) - λ * Σ_{W∈𝒲_LoRA} ||W||₂²`
ここで `𝒰` はユニークな `(プレフィックス, トークン)` ペアのセット、`λ` はLoRAパラメータに対する正則化係数です。S-GRPOと同様に、問題ごとに1回更新し、PPOスタイルのクリッピングは行いません。

**リプレイに基づくリサンプリング:** ほとんどの生成候補は序盤で多様化するため、生成の後続部分からの学習を助けるために、設定可能なリプレイ機構を導入しています。過去に生成された候補を報酬 `R` に基づいて成功 (`R ≥ r`) または失敗 (`R < r`) に分類します。更新ステップごとに、定義された予算 `C_success`, `C_failure` に従って成功または失敗した候補からサンプリングし、**プロンプトの終わりから候補の最後のトークンまでの範囲 `[T_prompt, T_total]` でランダムなトークン位置 `t` を選択**します。この位置 `t` までのプレフィックス `P` から生成を再開し、新しい生成候補を `|𝒢|` 個生成します。これらの新しい候補を用いて `(プレフィックス, トークン)` アドバンテージペアを収集し、ポリシーを更新します。

**2.3. フルGRPOベースライン**

トークンサンプリングの影響を分離するために、オリジナルGRPO目的関数（式3）を実装しました。こちらも問題ごとの更新回数は1回ですが、損失計算には**すべてのトークンを含めます**。メモリ制約に対応するため、**勾配累積**を用いてGRPOを適応させています。勾配を有効にせずに生成候補を生成し、その後各生成候補に対して順伝播・逆伝播を実行して勾配を累積します。

**3. 実験設定**

本研究では、**Qwen2-1.5Bモデル**を**LoRA**を用いてファインチューニングしました。アダプターパラメータのみを更新し、ベースウェイトは固定しました。LoRAモジュールは最終アテンション層のクエリおよびバリュー射影に挿入しました（SVAMPは最終1/3層、rank=16、掛け算は最終1/4層、rank=8）。学習は単一のA100 40GB GPU上でAdamWオプティマイザ、学習率 `1e-4`、正則化係数 `β=λ=0.01`、float32精度で行われました。

評価には以下の2つの推論ベンチマークを使用しました:
*   **SVAMP**: 数値量に対する言語的推論を必要とする単語算数問題のベンチマーク。
*   **多桁掛け算**: 複数桁の整数掛け算タスク。ランダムにサンプリングされた3桁の整数（101～999）の掛け算を使用しました。

モデルの応答は最大300トークンに制限し、温度は0.3としました。報酬は**厳密一致精度**に基づいており、モデル出力から抽出された**最後の整数**を予測値として扱いました。

効果的なバッチサイズは特に断りのない限り1とし、勾配累積はデフォルトで無効にしました（GRPOベースラインを除く）。S-GRPOおよびGRPOベースラインはプロンプトごとに **|𝒢|=8** の生成候補をサンプリングし、T-SPMOは **|𝒢|=50** の生成候補を使用しました。

**4. 結果**

SVAMPテストセットおよび生成された掛け算テストセットにおける精度を報告します。

**表I：SVAMPベンチマーク テストセット結果 (n=300)**
| 手法           | 精度 (%) |
| :------------- | :------- |
| Base Qwen2-1.5B | 45.0     |
| GRPO           | 46.7     |
| **S-GRPO**     | **70.3** |
| **T-SPMO**     | **71.6** |

SVAMPにおいて、**S-GRPOとT-SPMOの両方がベースモデルの性能を大幅に向上**させました。GRPOはベースモデルから有意な性能向上を示しませんでした。

**表II：3桁 x 3桁掛け算テストセット結果 (n=3000)**
| 手法           | 精度 (%) |
| :------------- | :------- |
| Base Qwen2-1.5B | 3.9      |
| GRPO           | 4.4      |
| S-GRPO         | 22.9     |
| **T-SPMO**     | **70.0** |

3桁 x 3桁掛け算タスクでは、ベースモデルおよびGRPOベースラインの精度は非常に低いものでした。S-GRPOは改善を示しましたが、T-SPMOは**両ベースラインおよびS-GRPOを圧倒的に凌駕し、精度70.0%を達成**しました。図1では、掛け算タスクにおけるT-SPMOとS-GRPOの性能の大きな乖離が明確に示されています。

アブレーション研究により、S-GRPOはSVAMPに対して比較的頑健でしたが、掛け算のような難しいタスクでは `α` や `k` のハイパーパラメータに敏感であることが示されました。また、S-GRPOでは効果的なバッチサイズを増やすと性能が低下しました。T-SPMOはSVAMPで全ての構成で高い性能を示しましたが、掛け算ではグループサイズ `|𝒢|` やリプレイ戦略に敏感であり、**グループサイズが大きいほど、また成功した生成結果からのリプレイを含むことが重要**であることが分かりました。

**5. 考察**

我々は、T-SPMOが多桁掛け算タスクでS-GRPOよりも優れた性能を示した理由について、各手法の**トークンレベルの信用割り当てと部分的な正しさの扱い方の違い**に起因すると仮説を立てています。S-GRPOはサンプリングされたトークンに対してグループレベルの更新を適用するため、孤立したエラーに敏感であり、有用な下位スキルが忘れられる原因となる可能性があると考えられます。一方、T-SPMOは**特定の `(プレフィックス, トークン)` ペアに対してアドバンテージを計算する**ため、後続にエラーがあっても正しい決定が強化されることを可能にします。このよりきめ細かい信用割り当てが、特に算術タスクにおけるT-SPMOの大きな性能向上を支えていると考えられます。

GRPOベースラインが性能向上しなかったのは、LoRAファインチューニング下で、S-GRPOと同様の、あるいはより顕著な感度問題を抱えている可能性があるためとしています。S-GRPOとT-SPMOのトレードオフとして、前者はグループレベルの一貫性を重視し、後者は選択的なトークンレベルの最適化を行います。

**6. 限界**

本研究にはいくつかの限界があります。
*   単一の40GB GPU制約のため、**フルモデルファインチューニングのベースラインとの比較は行っていません**。
*   GRPOベースラインは勾配累積によるウォールクロック時間の増加のため、**広範なハイパーパラメータ探索を行えていません**。したがって、より最適化されたGRPOが我々の手法を上回る可能性を完全に排除できません。
*   実験は**単一のベースモデル（Qwen2-1.5B）**に焦点を当てています。結果の汎用性については、今後の研究で検証が必要です。

**7. 今後の展望**

今後の研究の有望な方向性として、T-SPMOをMATHベンチマークのような、より長く、よりセマンティックに多様なChain-of-Thoughtを伴うタスクに適用することが挙げられます。表面的なトークンマッチングが不十分な場合、T-SPMOのプレフィックスマッチング機構を、潜在表現に基づくファジーマッチングやパラフレーズクラスタリングのような**セマンティックを意識した構造で拡張**することで、トークンレベルRLの汎化性を向上させ、語彙的な変動への感度を低減できる可能性があります。S-GRPOに関しては、損失に貢献するトークンのサンプリングアルゴリズムのバリエーションを探求することが考えられます。

**8. 結論**

本研究では、**メモリ制約下でLLMsの推論性能を向上させるために設計された2つのRL手法、S-GRPOとT-SPMO**を提案しました。両手法はクリティックフリーであり、LoRAファインチューニングフレームワーク内で動作するため、単一の40GB GPUでの学習に実用的です。SVAMPおよび多桁掛け算タスクにおいて、提案手法はゼロショットベースラインを大幅に上回り、**特にT-SPMOは算術タスクで非常に強力な結果**を達成しました。

フル軌道損失を用いるGRPOベースラインは、ベースモデルから有意な性能向上を示しませんでした。これは、パラメータ効率の良いファインチューニングを使用する場合、GRPOもS-GRPOと同様、あるいはより顕著な感度問題を示す可能性を示唆しています。このことは、制約のある設定下での注意深い報酬割り当ての重要性と、フルモデルRLアルゴリズムをLoRA設定に適応させる際の追加の安定化またはチューニングの必要性を強調しています。

本研究は、フルファインチューニングが実行不可能な推論タスクのためのRLを可能にするという補完的な問題設定を対象としており、限られたハードウェアで作業する研究者や実務家にとって重要なギャップを埋める軽量かつ効率的なアプローチを提供します。今後の作業では、これらの技術をよりセマンティックに多様なタスクに拡張し、トークンレベル最適化の効率ときめ細かい信用割り当ての利点を保持しつつスケールアップする戦略を研究する予定です。

---
